{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-15 13:38:55.685672: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-15 13:38:55.813779: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-01-15 13:38:55.813794: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-01-15 13:38:56.521543: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-15 13:38:56.521608: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-01-15 13:38:56.521615: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.mobilenet_v2(pretrained=True, progress=False)\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 1000\n"
     ]
    }
   ],
   "source": [
    "X, y = shap.datasets.imagenet50() # y nie będzie potrzebne, zawiera odwołania do obrazów, które tu nie są wykorzystywane\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "with open(shap.datasets.cache(url)) as file:\n",
    "    class_names = [v[1] for v in json.load(file).values()]\n",
    "print(len(X),len(class_names)) # sprawdzamy czy się udało: 50, 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja rotująca kształt tensora, w zależności od tego czy mamy kolekcję obrazów (4 wymiary, n,color,height,width) czy trzy (color,height,width). Funkcja pozostawia n bez zmian (jeśli jest) i przekształca klasyczny obraz (kanał, wiersz, kolumna)  poprzez przeniesienie c na koniec, tak jak w definicji tensora\n",
    "```\n",
    "  n number\n",
    "| c color\n",
    "| h height rows\n",
    "V w width cols\n",
    "\n",
    "n\n",
    "h\n",
    "w\n",
    "c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja sprawdza czy mamy 4 wymiary (wiele obrazów) czy trzy wimiary (jeden obraz)\n",
    "\n",
    "def nhwc_to_nchw(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        x = x if x.shape[1] == 3 else x.permute(0, 3, 1, 2)\n",
    "    elif x.dim() == 3:\n",
    "        x = x if x.shape[0] == 3 else x.permute(2, 0, 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def nchw_to_nhwc(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.dim() == 4:\n",
    "        x = x if x.shape[3] == 3 else x.permute(0, 2, 3, 1)\n",
    "    elif x.dim() == 3:\n",
    "        x = x if x.shape[2] == 3 else x.permute(1, 2, 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natępnie ręcznie obrazy są skalowane do przedziału (0,1) a następnie standaryzowane, względem wartości średnich dla kanałów dla pretrenowanej sieci. Proces ten jest połączony w potok (pipe) metodą `Compose`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Rotacja jest potrzebna aby zastosować transformację obrazu w sposób zwektoryzowany\n",
    "transform = [\n",
    "    torchvision.transforms.Lambda(nhwc_to_nchw),\n",
    "    torchvision.transforms.Lambda(lambda x: x * (1 / 255)),\n",
    "    torchvision.transforms.Normalize(mean=mean, std=std),\n",
    "    torchvision.transforms.Lambda(nchw_to_nhwc),\n",
    "]\n",
    "\n",
    "# transformacja odwrotna służy do wykonania destandaryzacji\n",
    "inv_transform = [\n",
    "    torchvision.transforms.Lambda(nhwc_to_nchw),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=(-1 * np.array(mean) / np.array(std)).tolist(),\n",
    "        std=(1 / np.array(std)).tolist(),\n",
    "    ),\n",
    "    torchvision.transforms.Lambda(nchw_to_nhwc),\n",
    "]\n",
    "\n",
    "#połączenie w pipe\n",
    "transform = torchvision.transforms.Compose(transform)\n",
    "inv_transform = torchvision.transforms.Compose(inv_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja `predict`, zwraca zawiera obraz (np.array) na tensor, zgodny z urządzeniem (device na którym wykonywana jest predykcja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img: np.ndarray) -> torch.Tensor:\n",
    "    img = nhwc_to_nchw(torch.Tensor(img))\n",
    "    img = img.to(device)\n",
    "    output = model(img)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [643]: ['mask']\n"
     ]
    }
   ],
   "source": [
    "Xtr = transform(torch.Tensor(X))\n",
    "out = predict(Xtr[7:8])\n",
    "classes = torch.argmax(out, axis=1).cpu().numpy() # używam cpu bo mi CUDA nie działa na Waylandzie\n",
    "print(f\"Classes: {classes}: {np.array(class_names)[classes]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budowanie właściwego explainera oraz maskera. Używamy obrazu rozmytego kernelem 64,64 na obrazie o ksztłcie kolekcji obrazów (obliczenia trochę trwają)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "masker_blur = shap.maskers.Image(\"blur(64,64)\", Xtr[0].shape)\n",
    "# funkcja predict, masker output, lista nazw klas\n",
    "explainer = shap.Explainer(predict, masker_blur, output_names=class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
